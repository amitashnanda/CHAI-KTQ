{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08f62e1e-6211-4fe1-a1dd-c9a42401c31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /tmp/xdg-cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1daadab7baf4d3b98dfa77a0ac20e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLaMA 2 Model Loaded Successfully\n",
      "✅ LLaMA 2 Model Loaded Successfully\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot handle batch sizes > 1 if no padding token is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 330\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# model, tokenizer = load_opt_classifier(model_name)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m size1 \u001b[38;5;241m=\u001b[39m get_model_size(model)\n\u001b[0;32m--> 330\u001b[0m (a1,b1)\u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_piqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m model \u001b[38;5;241m=\u001b[39m apply_pruning(model,tokenizer,dataset_name)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mprint\u001b[39m(a1)\n",
      "Cell \u001b[0;32mIn[18], line 121\u001b[0m, in \u001b[0;36mevaluate_model_piqa\u001b[0;34m(model, tokenizer, dataset_name)\u001b[0m\n\u001b[1;32m    112\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m    113\u001b[0m     [prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m choice \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices],\n\u001b[1;32m    114\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m    118\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# ✅ Ensure inputs are on the correct device\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ✅ No need to move the model to device again\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    123\u001b[0m predicted_choice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1380\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle batch sizes > 1 if no padding token is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1382\u001b[0m     sequence_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot handle batch sizes > 1 if no padding token is defined."
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "from chai_quant import chai_quant_enhancement\n",
    "from chai_kd import chai_knowledge_distillation_enhancement\n",
    "from chai_target import main_chai_target\n",
    "from original_chai import apply_pruning\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\" Temporarily saves the model and measures its file size in MB. \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "        temp_path = temp_file.name\n",
    "    \n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    size_mb = os.path.getsize(temp_path) / (1024 * 1024)  # Convert bytes to MB\n",
    "    os.remove(temp_path)  #  Clean up temporary file\n",
    "    return size_mb\n",
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"Extracts attention scores from the model while ensuring correct dimensions.\"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = input_ids.to(device)  #  Move input IDs to GPU\n",
    "    model.to(device)\n",
    "    \n",
    "    attention_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        if hasattr(outputs, \"attentions\") and outputs.attentions is not None:\n",
    "            for layer_idx, attn in enumerate(outputs.attentions):\n",
    "                attn = attn.cpu().numpy()  # Move to CPU\n",
    "                if attn.ndim == 4:  # Expected shape: (batch_size, num_heads, seq_length, seq_length)\n",
    "                    attn = np.mean(attn, axis=(0, 2, 3))  #  Average across heads and sequences\n",
    "                elif attn.ndim == 3:  # Unexpected case, still averaging\n",
    "                    attn = np.mean(attn, axis=(0, 2))\n",
    "                elif attn.ndim == 2:  # More unexpected cases\n",
    "                    attn = np.mean(attn, axis=0)\n",
    "                \n",
    "                attention_scores[layer_idx] = attn  #  Store correctly processed attention scores\n",
    "\n",
    "        else:\n",
    "            return {\"error\": \"Model does not return attention scores. Check model architecture.\"}\n",
    "\n",
    "    return attention_scores\n",
    "\n",
    "def divide_layers_by_sensitivity(sensitivities):\n",
    "    \"\"\" Splits layers into 3 groups (High, Medium, Low) based on sensitivity scores. \"\"\"\n",
    "    sorted_layers = sorted(sensitivities, key=sensitivities.get, reverse=True)\n",
    "    num_layers = len(sorted_layers)\n",
    "    high = sorted_layers[: int(num_layers * 0.2)]\n",
    "    medium = sorted_layers[int(num_layers * 0.2) : int(num_layers * 0.7)]\n",
    "    low = sorted_layers[int(num_layers * 0.7) :]\n",
    "    return high, medium, low\n",
    "\n",
    "def apply_mixed_precision(model, medium, low):\n",
    "    \"\"\" Applies mixed precision quantization to the model. \"\"\"\n",
    "    for layer_idx in medium + low:\n",
    "        for param in model.model.decoder.layers[layer_idx].parameters():\n",
    "            param.data = param.data.half()\n",
    "    model.half()\n",
    "    return model\n",
    "def load_opt_classifier(model_name):\n",
    "    \"\"\"Load the specified OPT model and tokenizer.\"\"\"\n",
    "    model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "def compute_sensitivity(attention_scores):\n",
    "    cleaned_scores = {\n",
    "        layer_idx: np.mean(np.abs(np.array(scores, dtype=np.float32)))\n",
    "        for layer_idx, scores in attention_scores.items()\n",
    "        if isinstance(scores, (list, np.ndarray))\n",
    "    }\n",
    "    return cleaned_scores\n",
    "def evaluate_model_piqa(model, tokenizer):\n",
    "    \"\"\" Evaluates model accuracy on PIQA dataset (multiple-choice task). \"\"\"\n",
    "    dataset = load_dataset(\"piqa\", split=\"validation[:100]\")\n",
    "\n",
    "    model.eval()  #  Remove `.to(device)`\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            prompt = example[\"goal\"]\n",
    "            choices = [example[\"sol1\"], example[\"sol2\"]]\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                [prompt + \" \" + choice for choice in choices],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(device)  #  Ensure inputs are on the correct device\n",
    "\n",
    "            #  No need to move the model to device again\n",
    "            outputs = model(**inputs, use_cache=False)\n",
    "            logits = outputs.logits.squeeze()\n",
    "            predicted_choice = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            label = torch.tensor(example[\"label\"], device=device)\n",
    "            correct += (predicted_choice == label).sum().item()\n",
    "            total += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    return (correct / total) * 100, latency\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Evaluation -------------------- #\n",
    "def evaluate_model_piqa1(model, tokenizer):\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"piqa\", split=\"validation\")\n",
    "    #  Dataset mappings for correct loading\n",
    "    # dataset_mapping = {\n",
    "    #     \"sst2\": (\"glue\", \"sst2\", \"validation\", \"sentence\"),\n",
    "    #     \"rte\": (\"glue\", \"rte\", \"validation\", (\"sentence1\", \"sentence2\")),\n",
    "    #     \"piqa\": (\"piqa\", None, \"validation\", (\"goal\", \"sol1\", \"sol2\")),  # PIQA has different format\n",
    "    # }\n",
    "\n",
    "    # if dataset_name not in dataset_mapping:\n",
    "    #     return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    # dataset_source, dataset_subset, split_name, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    # #  Load dataset with cache handling\n",
    "    # try:\n",
    "    #     dataset = load_dataset(dataset_source, dataset_subset, split=split_name, cache_dir=\"./cache\") if dataset_subset \\\n",
    "    #         else load_dataset(dataset_source, split=split_name, cache_dir=\"./cache\")\n",
    "    # except Exception as e:\n",
    "    #     return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    #  Enable GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #  Ensure correct precision (avoid float16 on CPU)\n",
    "    if device.type == \"cpu\":\n",
    "        model.to(torch.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if dataset_name == \"piqa\":\n",
    "                #  Correctly tokenize both solutions separately\n",
    "                inputs1 = tokenizer(batch[\"goal\"], batch[\"sol1\"], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                inputs2 = tokenizer(batch[\"goal\"], batch[\"sol2\"], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "                #  Get logits separately for each solution\n",
    "                outputs1 = model(**inputs1, use_cache=False).logits\n",
    "                outputs2 = model(**inputs2, use_cache=False).logits\n",
    "\n",
    "                #  Predict the solution with higher confidence\n",
    "                predictions = torch.argmax(torch.stack([outputs1, outputs2], dim=-1), dim=-1).squeeze()\n",
    "            else:\n",
    "                #  Handle other datasets (SST2, RTE) normally\n",
    "                if isinstance(text_key, tuple):\n",
    "                    inputs = tokenizer(*[batch[key] for key in text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                else:\n",
    "                    inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.argmax(F.softmax(outputs.logits, dim=-1), dim=-1)\n",
    "\n",
    "            labels = torch.tensor(batch[\"label\"], dtype=torch.long).to(device, non_blocking=True)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%, Latency: {latency:.4f} sec\")\n",
    "    return (accuracy, latency)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "\n",
    "    #  Dataset mappings for correct loading\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"validation\", \"sentence\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"validation\", (\"sentence1\", \"sentence2\")),  # RTE has two sentences\n",
    "        \"piqa\": (\"piqa\", None, \"validation\", (\"goal\", \"sol1\", \"sol2\")),  # PIQA has different format\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, split_name, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    # Load dataset with cache handling\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=split_name, cache_dir=\"./cache\") if dataset_subset \\\n",
    "            else load_dataset(dataset_source, split=split_name, cache_dir=\"./cache\")\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    #  Enable GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #  Ensure correct precision (avoid float16 on CPU)\n",
    "    if device.type == \"cpu\":\n",
    "        model.to(torch.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #  Handle multiple input fields correctly\n",
    "            if isinstance(text_key, tuple):  # RTE & PIQA\n",
    "                inputs = tokenizer(*[batch[key] for key in text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            else:\n",
    "                inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            #  FIX: Correct tensor creation and movement\n",
    "            labels = torch.tensor(batch[\"label\"], dtype=torch.long).to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(F.softmax(outputs.logits, dim=-1), dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "    print([accuracy, latency])\n",
    "    print(\"[accuracy, latency]\")\n",
    "    a = accuracy\n",
    "    b = latency\n",
    "    return (a, b)\n",
    "\n",
    "def chai_quant_enhancement(chai_base_model,tokenizer,dataset_name):\n",
    "    #  Save & Reload to Apply Pruning\n",
    "    #  Measure Model Size After Pruning\n",
    "    input_ids = torch.randint(0, 50256, (1, 32))\n",
    "    attention_scores = get_attention_scores(chai_base_model, input_ids)\n",
    "    sensitivities = compute_sensitivity(attention_scores)\n",
    "    high, medium, low = divide_layers_by_sensitivity(sensitivities)\n",
    "\n",
    "    #  Apply Mixed Precision Quantization (CHAI-Quant)\n",
    "    print(\"\\n Applying Mixed Precision Quantization (CHAI-Quant)...\")\n",
    "    chai_quant_model = apply_mixed_precision(chai_base_model, medium, low)\n",
    "    print(\" [chai-quant] After modification: model parameters\")\n",
    "    for name, param in chai_quant_model.named_parameters():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "    return chai_quant_model\n",
    "# dataset_name = \"piqa\"\n",
    "# model_name = \"facebook/opt-2.7b\"\n",
    "# model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "#  Log in to Hugging Face (ensure your token has \"read\" access)\n",
    "HUGGINGFACE_TOKEN = \"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\"  # Replace with your actual token\n",
    "login(HUGGINGFACE_TOKEN)\n",
    "\n",
    "dataset_name = \"piqa\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  #  Use LLaMA-2 model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "#  Define the LLaMA 2 model name\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "#  Load model with automatic device placement\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\",  #  Automatically assigns model to available GPU(s) & CPU\n",
    "    torch_dtype=torch.float16,  #  Use half-precision for efficiency\n",
    "    trust_remote_code=True  #  Needed for some HF models\n",
    ")\n",
    "\n",
    "#  Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\" LLaMA 2 Model Loaded Successfully\")\n",
    "\n",
    "#  Load tokenizer with padding setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  #  Set EOS token as pad token\n",
    "tokenizer.padding_side = \"left\"  #  Important for LLaMA models\n",
    "\n",
    "print(\" LLaMA 2 Model Loaded Successfully\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model, tokenizer = load_opt_classifier(model_name)\n",
    "size1 = get_model_size(model)\n",
    "(a1,b1)= evaluate_model_piqa(model, tokenizer, dataset_name)\n",
    "\n",
    "model = apply_pruning(model,tokenizer,dataset_name)\n",
    "\n",
    "print(a1)\n",
    "print(b1)\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "print(\"chai base \")\n",
    "size2 = get_model_size(model)\n",
    "print(\"reduction percentage\")\n",
    "print((size1-size2)*100/size1)\n",
    "(a,b)= evaluate_model_piqa(model, tokenizer, dataset_name)\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"------------------------------------------------------------------\")\n",
    "model = chai_quant_enhancement(model, tokenizer, dataset_name)\n",
    "size3 = get_model_size(model)\n",
    "\n",
    "print(\"applied_methods.append(Quantization)\")\n",
    "(a,b)= evaluate_model_piqa(model, tokenizer, dataset_name)\n",
    "print(\"Accuracy after applying methods\")\n",
    "print(a)\n",
    "print(\"latency\")\n",
    "print(b)\n",
    "print(\"reduction percentage\")\n",
    "print((size2-size3)*100/size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9024a7b0-ba77-47d0-b0fa-0e602667cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "print(\"Sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa61877-8069-4ed8-9f1e-0561b5e85428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
