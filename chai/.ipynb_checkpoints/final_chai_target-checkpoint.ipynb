{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aae26aa-941d-4f3a-a9c8-8c460541a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 21:30:39.295210: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-15 21:30:39.295231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-15 21:30:39.296304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 21:30:39.302379: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_opt_classifier(model_name):\n",
    "    \"\"\"Load the specified OPT model and tokenizer.\"\"\"\n",
    "    model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_model_size(model, path=\"temp_model.pth\"):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    os.remove(path)\n",
    "    return size_mb\n",
    "def enforce_head_constraint(num_heads, embed_dim):\n",
    "    \"\"\" Adjusts number of heads to ensure divisibility with embedding dimension. \"\"\"\n",
    "    while embed_dim % num_heads != 0:\n",
    "        num_heads -= 1\n",
    "    return num_heads\n",
    "\n",
    "def load_opt_classifier():\n",
    "    model = OPTForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "    return model, tokenizer\n",
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\" Extracts attention scores from the model while ensuring correct dimensions. \"\"\"\n",
    "    attention_scores = {}\n",
    "    input_ids = input_ids.to(device)  #  Move input IDs to GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "\n",
    "        for layer_idx, attn in enumerate(outputs.attentions):\n",
    "            attn = attn.cpu().numpy()  #  Move to CPU\n",
    "            attn = np.mean(attn, axis=(0, 2, 3)) if attn.ndim == 4 else np.mean(attn, axis=0)\n",
    "            attention_scores[layer_idx] = attn  #  Store correctly processed attention scores\n",
    "\n",
    "    return attention_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cluster_heads(attention_scores, num_clusters):\n",
    "    \"\"\" Clusters attention heads while ensuring correct shape. \"\"\"\n",
    "    num_heads = len(attention_scores)\n",
    "\n",
    "    if num_heads <= 10:\n",
    "        return list(range(num_heads))\n",
    "\n",
    "    attention_scores = np.array(attention_scores).reshape(-1, 1)  #  Flatten for clustering\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=\"auto\")\n",
    "    kmeans.fit(attention_scores)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    cluster_representatives = []\n",
    "\n",
    "    for cluster_idx in range(num_clusters):\n",
    "        indices = np.where(labels == cluster_idx)[0]\n",
    "        if len(indices) > 0:\n",
    "            keep_count = max(1, len(indices) * 5 // 10)  #  Pruning 50% of heads per cluster\n",
    "            cluster_representatives.extend(indices[:keep_count])\n",
    "\n",
    "    return sorted(cluster_representatives)\n",
    "\n",
    "def prune_attention_heads(model, clustered_heads):\n",
    "    \"\"\" Prunes attention heads while ensuring correct embedding dimensions. \"\"\"\n",
    "    for layer_idx, heads_to_keep in enumerate(clustered_heads):\n",
    "        attn_layer = model.model.decoder.layers[layer_idx].self_attn\n",
    "\n",
    "        #  Ensure valid number of heads per layer\n",
    "        original_num_heads = attn_layer.num_heads\n",
    "        new_num_heads = enforce_head_constraint(len(heads_to_keep), attn_layer.embed_dim)\n",
    "\n",
    "        #  Update number of heads\n",
    "        attn_layer.num_heads = new_num_heads\n",
    "\n",
    "        #  Ensure Q, K, V projections match new number of heads\n",
    "        head_dim = attn_layer.embed_dim // original_num_heads\n",
    "        new_embed_dim = new_num_heads * head_dim\n",
    "\n",
    "        attn_layer.q_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "        attn_layer.k_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "        attn_layer.v_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "\n",
    "        #  Ensure output projection layer matches new size\n",
    "        attn_layer.out_proj = nn.Linear(new_embed_dim, attn_layer.embed_dim, bias=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def divide_layers_by_sensitivity(sensitivities):\n",
    "    \"\"\" Splits layers into 3 groups (High, Medium, Low) based on sensitivity scores. \"\"\"\n",
    "    sorted_layers = sorted(sensitivities, key=sensitivities.get, reverse=True)\n",
    "    num_layers = len(sorted_layers)\n",
    "    high = sorted_layers[: int(num_layers * 0.2)]\n",
    "    medium = sorted_layers[int(num_layers * 0.2) : int(num_layers * 0.7)]\n",
    "    low = sorted_layers[int(num_layers * 0.7) :]\n",
    "    return high, medium, low\n",
    "\n",
    "def apply_mixed_precision(model, medium, low):\n",
    "    \"\"\" Applies mixed precision quantization to the model. \"\"\"\n",
    "    for layer_idx in medium + low:\n",
    "        for param in model.model.decoder.layers[layer_idx].parameters():\n",
    "            param.data = param.data.half()\n",
    "    model.half()\n",
    "    return model\n",
    "\n",
    "def compute_sensitivity(attention_scores):\n",
    "    # Debugging: Print attention scores\n",
    "    print(\"Raw Attention Scores:\", attention_scores)\n",
    "\n",
    "    # Compute absolute mean for each layer\n",
    "    cleaned_scores = {\n",
    "        layer_idx: np.mean(np.abs(np.array(scores, dtype=np.float32)))\n",
    "        for layer_idx, scores in attention_scores.items()\n",
    "        if isinstance(scores, (list, np.ndarray)) and len(scores) > 0  # Ensure non-empty values\n",
    "    }\n",
    "    \n",
    "    # Debugging: Print computed sensitivities\n",
    "    print(\"Computed Sensitivities:\", cleaned_scores)\n",
    "    \n",
    "    return cleaned_scores# -------------------- Evaluation -------------------- #\n",
    "# def evaluate_model(model, tokenizer):\n",
    "#     \"\"\" Evaluates model accuracy on PIQA dataset (multiple-choice task). \"\"\"\n",
    "#     dataset = load_dataset(\"piqa\", split=\"validation[:100]\")\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for example in dataset:\n",
    "#             prompt = example[\"goal\"]\n",
    "#             choices = [example[\"sol1\"], example[\"sol2\"]]\n",
    "#             inputs = tokenizer([prompt + \" \" + choice for choice in choices], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits.squeeze()\n",
    "#             predicted_choice = torch.argmax(logits).item()\n",
    "#             correct += (predicted_choice == example[\"label\"])\n",
    "#             total += 1\n",
    "\n",
    "#     return (correct / total) * 100\n",
    "\n",
    "# -------------------- Main Execution -------------------- #\n",
    "def get_optimal_clusters(attention_scores):\n",
    "    \"\"\" Determines optimal clusters for attention heads using the Elbow Method. \"\"\"\n",
    "    num_heads = len(attention_scores)\n",
    "    if num_heads <= 10:\n",
    "        return num_heads\n",
    "    max_clusters = max(num_heads - int(0.2 * num_heads), num_heads * 4 // 5)\n",
    "    errors = []\n",
    "    cluster_range = range(1, max_clusters + 1)\n",
    "    for num_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=\"auto\")\n",
    "        kmeans.fit(attention_scores.reshape(-1, 1))\n",
    "        errors.append(kmeans.inertia_)\n",
    "    elbow = KneeLocator(cluster_range, errors, curve=\"convex\", direction=\"decreasing\")\n",
    "    return max(num_heads - int(0.2 * num_heads), elbow.elbow if elbow.elbow else max_clusters)\n",
    "\n",
    "def get_model_size(model, path=\"temp_model.pth\"):\n",
    "    \"\"\" Saves model temporarily and checks disk size. \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)  # Convert bytes to MB\n",
    "    os.remove(path)  #  Clean up after measurement\n",
    "    return size_mb\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"sentence\"),\n",
    "        \"piqa\": (\"piqa\", \"train\", \"goal\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"sentence1\"),\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=\"train\", trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            inputs = tokenizer(example[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions.item() == example[\"label\"])\n",
    "            total += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "    return accuracy, latency\n",
    "\n",
    "def apply_pruning(model, tokenizer,dataset_name):\n",
    "    size_before = get_model_size(model)\n",
    "\n",
    "    print(\"\\n Evaluating Accuracy Before Any Modification...\")\n",
    "    # accuracy_before = evaluate_model(model, tokenizer,dataset_name)\n",
    "    # print(f\" Original Accuracy: {accuracy_before:.2f}%\\n\")\n",
    "\n",
    "    #  Measure Original Model Size\n",
    "    #  Compute Attention Scores\n",
    "    print(\"\\n Computing Attention Scores...\")\n",
    "    input_ids = torch.randint(0, 50256, (1, 32))  # Random input for attention extraction\n",
    "    attention_scores = get_attention_scores(model, input_ids)\n",
    "\n",
    "    #  Apply Clustering to All Layers\n",
    "    print(\"\\n Clustering Attention Heads...\")\n",
    "#  Ensure layers exist before accessing\n",
    "    if not attention_scores:\n",
    "        raise ValueError(\" No attention scores extracted! Check if model supports output_attentions.\")\n",
    "\n",
    "    available_layers = list(attention_scores.keys())\n",
    "    print(f\" Available Layers for Clustering: {available_layers}\")\n",
    "\n",
    "    clustered_heads = [\n",
    "        cluster_heads(attention_scores[layer], get_optimal_clusters(attention_scores[layer]))\n",
    "        for layer in available_layers\n",
    "    ]\n",
    "\n",
    "\n",
    "    #  Apply Clustering & Pruning (CHAI-Base)\n",
    "    print(\"\\n Applying Clustering and Pruning (CHAI-Base)...\")\n",
    "    chai_base_model = prune_attention_heads(model, clustered_heads)\n",
    "    print(\"got heads\")\n",
    "    return chai_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375cc763-0f6f-4da3-9dcf-5f6eee6b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def compute_sensitivity(attention_scores):\n",
    "    # Debugging: Print attention scores\n",
    "    print(\"Raw Attention Scores:\", attention_scores)\n",
    "\n",
    "    # Compute absolute mean for each layer\n",
    "    cleaned_scores = {\n",
    "        layer_idx: np.mean(np.abs(np.array(scores, dtype=np.float32)))\n",
    "        for layer_idx, scores in attention_scores.items()\n",
    "        if isinstance(scores, (list, np.ndarray)) and len(scores) > 0  # Ensure non-empty values\n",
    "    }\n",
    "    \n",
    "    # Debugging: Print computed sensitivities\n",
    "    print(\"Computed Sensitivities:\", cleaned_scores)\n",
    "    \n",
    "    return cleaned_scores\n",
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"Extracts attention scores from the model while ensuring correct dimensions.\"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = input_ids.to(device)  #  Move input IDs to GPU\n",
    "    model.to(device)\n",
    "    \n",
    "    attention_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        if hasattr(outputs, \"attentions\") and outputs.attentions is not None:\n",
    "            for layer_idx, attn in enumerate(outputs.attentions):\n",
    "                attn = attn.cpu().numpy()  #  Move to CPU\n",
    "                if attn.ndim == 4:  # Expected shape: (batch_size, num_heads, seq_length, seq_length)\n",
    "                    attn = np.mean(attn, axis=(0, 2, 3))  #  Average across heads and sequences\n",
    "                elif attn.ndim == 3:  # Unexpected case, still averaging\n",
    "                    attn = np.mean(attn, axis=(0, 2))\n",
    "                elif attn.ndim == 2:  # More unexpected cases\n",
    "                    attn = np.mean(attn, axis=0)\n",
    "                \n",
    "                attention_scores[layer_idx] = attn  #  Store correctly processed attention scores\n",
    "\n",
    "        else:\n",
    "            return {\"error\": \"Model does not return attention scores. Check model architecture.\"}\n",
    "\n",
    "    return attention_scores\n",
    "def main_chai_target(model, tokenizer, dataset_name, epochs=30, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Identifies and perturbs the most sensitive layers of the model based on accuracy drop.\n",
    "    Returns the modified model with targeted layers fine-tuned.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The pre-trained model to analyze.\n",
    "        tokenizer: Tokenizer associated with the model.\n",
    "        dataset_name (str): Name of the dataset to use ('rte', 'piqa', or 'sst2').\n",
    "        epochs (int, optional): Number of fine-tuning epochs. Defaults to 3.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 16.\n",
    "        learning_rate (float, optional): Learning rate for fine-tuning. Defaults to 2e-5.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model after targeted fine-tuning on sensitive layers.\n",
    "    \"\"\"\n",
    "    print(\" [chai-target] Before modification: model parameters\")\n",
    "    for name, param in model.named_parameters():\n",
    "      print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "# If model is in half precision and running on CPU, convert to float32\n",
    "    if device.type == \"cpu\":\n",
    "        model = model.float()  #  Converts model weights to float32 for CPU compatibility\n",
    "\n",
    "    # Mapping dataset names to Hugging Face's dataset format\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\"),\n",
    "        \"rte\": (\"glue\", \"rte\"),\n",
    "        \"piqa\": (\"piqa\", \"validation\"),\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' is not supported.\")\n",
    "\n",
    "    dataset_source, dataset_subset = dataset_mapping[dataset_name]\n",
    "    dataset = load_dataset(dataset_source, dataset_subset, split=\"train\")\n",
    "\n",
    "    # Determine the appropriate text column\n",
    "    text_column = 'sentence' if 'sentence' in dataset.column_names else 'goal'\n",
    "\n",
    "    # Preprocessing function for tokenization\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[text_column], truncation=True, padding='max_length', max_length=512)  \n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    train_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Identify the most sensitive layers\n",
    "    sensitivities = []\n",
    "\n",
    "    # Placeholder: Manually selected sensitive layers\n",
    "    input_ids = torch.randint(0, 50256, (1, 32))\n",
    "    attention_scores = get_attention_scores(model, input_ids)\n",
    "    sensitivities = compute_sensitivity(attention_scores)\n",
    "# Compute the threshold for top 30% (Only if sensitivities are not empty)\n",
    "    if sensitivities:\n",
    "        num_layers = len(sensitivities)\n",
    "\n",
    "        # Compute the threshold for top 30%\n",
    "        top_k = int(0.3 * num_layers)\n",
    "\n",
    "        # Get the indices of the top 30% most sensitive layers\n",
    "        top_layer_indices = torch.argsort(sensitivities, descending=True)[:top_k]\n",
    "        targeted_layers= top_layer_indices.tolist()\n",
    "        print(f\"Identified Targeted Layers: {targeted_layers}\")\n",
    "    else:\n",
    "        print(\" Warning: No sensitivities detected. Skipping targeted fine-tuning.\")\n",
    "        targeted_layers = [2,3,4]  #  Skip fine-tuning if no layers are detected\n",
    "\n",
    "    print(f\"Identified Targeted Layers: {targeted_layers}\")\n",
    "\n",
    "\n",
    "    # ðŸ”¹ **Fine-Tune Only Targeted Layers**\n",
    "    print(\"Starting fine-tuning on targeted layers...\")\n",
    "\n",
    "    # Freeze all layers except the targeted layers\n",
    "    for layer_num, layer in enumerate(model.model.decoder.layers):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = layer_num in targeted_layers  # Only fine-tune targeted layers\n",
    "\n",
    "        # Define training arguments\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\" if dataset_name != \"piqa\" else \"no\",  #  Fix for missing eval dataset\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,  #  Ensuring evaluation dataset is provided\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Ensure model is correctly returned\n",
    "    if hasattr(trainer.model, \"module\"):\n",
    "        return trainer.model.module  # Extract if wrapped in DataParallel\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "    return trainer.model  # Returning the fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc68ec97-e4b9-465e-a4ad-48a18f36cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ [chai-target] Before modification: model parameters\n",
      "  model.decoder.embed_tokens.weight: torch.Size([50272, 512])\n",
      "  model.decoder.embed_positions.weight: torch.Size([2050, 1024])\n",
      "  model.decoder.project_out.weight: torch.Size([512, 1024])\n",
      "  model.decoder.project_in.weight: torch.Size([1024, 512])\n",
      "  model.decoder.layers.0.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.0.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.0.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.0.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.0.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.1.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.1.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.1.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.1.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.2.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.2.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.2.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.2.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.3.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.3.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.3.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.3.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.4.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.4.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.4.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.4.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.5.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.5.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.5.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.5.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.6.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.6.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.6.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.6.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.7.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.7.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.7.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.7.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.8.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.8.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.8.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.8.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.9.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.9.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.9.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.9.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.10.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.10.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.10.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.10.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.11.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.11.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.11.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.11.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.12.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.12.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.12.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.12.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.13.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.13.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.13.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.13.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.14.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.14.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.14.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.14.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.15.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.15.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.15.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.15.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.16.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.16.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.16.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.16.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.17.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.17.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.17.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.17.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.18.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.18.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.18.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.18.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.19.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.19.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.19.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.19.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.20.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.20.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.20.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.20.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.21.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.21.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.21.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.21.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.22.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.22.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.22.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.22.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.23.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.23.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.23.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.23.final_layer_norm.bias: torch.Size([1024])\n",
      "  score.weight: torch.Size([2, 512])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160ffbfe0b6e4f39a9fcb75c68a18186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Attention Scores: {'error': 'Model does not return attention scores. Check model architecture.'}\n",
      "Computed Sensitivities: {}\n",
      "âš ï¸ Warning: No sensitivities detected. Skipping targeted fine-tuning.\n",
      "Identified Targeted Layers: [2, 3, 4]\n",
      "Starting fine-tuning on targeted layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='126300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     2/126300 : < :, Epoch 0.00/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 68.69 MiB is free. Process 3532101 has 1.57 GiB memory in use. Process 3532317 has 22.03 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 288.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m OPTForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmain_chai_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, tokenizer, dataset_name):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates model accuracy on the given dataset.\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m, in \u001b[0;36mmain_chai_target\u001b[0;34m(model, tokenizer, dataset_name, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    152\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    153\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    154\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    155\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[1;32m    156\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,  \u001b[38;5;66;03m# âœ… Ensuring evaluation dataset is provided\u001b[39;00m\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m    165\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 68.69 MiB is free. Process 3532101 has 1.57 GiB memory in use. Process 3532317 has 22.03 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 288.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "dataset_name = \"sst2\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = main_chai_target(model, tokenizer, dataset_name)\n",
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "\n",
    "    #  Dataset mappings for correct loading\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"validation\", \"sentence\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"validation\", (\"sentence1\", \"sentence2\")),  # RTE has two sentences\n",
    "        \"piqa\": (\"piqa\", None, \"validation\", (\"goal\", \"sol1\", \"sol2\")),  # PIQA has different format\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, split_name, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    #  Load dataset with cache handling\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=split_name, cache_dir=\"./cache\") if dataset_subset \\\n",
    "            else load_dataset(dataset_source, split=split_name, cache_dir=\"./cache\")\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    #  Enable GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #  Ensure correct precision (avoid float16 on CPU)\n",
    "    if device.type == \"cpu\":\n",
    "        model.to(torch.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #  Handle multiple input fields correctly\n",
    "            if isinstance(text_key, tuple):  # RTE & PIQA\n",
    "                inputs = tokenizer(*[batch[key] for key in text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            else:\n",
    "                inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            #  FIX: Correct tensor creation and movement\n",
    "            labels = torch.tensor(batch[\"label\"], dtype=torch.long).to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(F.softmax(outputs.logits, dim=-1), dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    return [accuracy, latency]\n",
    "\n",
    "dataset_name = \"piqa\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "[a1, b1] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "\n",
    "\n",
    "#  Apply Pruning\n",
    "model = apply_pruning(model, tokenizer, dataset_name)\n",
    "\n",
    "print(\"\\n Initial Model Evaluation:\")\n",
    "print(f\"Accuracy: {a1:.2f}%\")\n",
    "print(f\"Latency: {b1:.4f} sec\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#  Measure Size After Pruning\n",
    "print(\"\\n Chai Base Model Evaluation (After Pruning)\")\n",
    "size2 = get_model_size(model)\n",
    "print(f\"Reduction Percentage: {(size1 - size2) * 100 / size1:.2f}%\")\n",
    "\n",
    "[a, b] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "print(f\"Accuracy: {a:.2f}%\")\n",
    "print(f\"Latency: {b:.4f} sec\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#  Apply CHAI-Quant Enhancement\n",
    "model = main_chai_target(model, tokenizer)\n",
    "size3 = get_model_size(model)\n",
    "print(\"\\n Applied Methods: TARGETTED FINE TUNING\")\n",
    "[a, b] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "\n",
    "print(\"\\n Final Model Evaluation (After CHAI-TARGET)\")\n",
    "print(f\"Accuracy after applying methods: {a:.2f}%\")\n",
    "print(f\"Latency: {b:.4f} sec\")\n",
    "print(f\"Reduction Percentage: {(size2 - size3) * 100 / size2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93205403-2992-4dda-9519-31abff9612d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
