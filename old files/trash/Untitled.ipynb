{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a095dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49f3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ed19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438d427b1a3348d982c561124035d0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32384684f8ac4569a6a55ca9dba50158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c227f1718d9742b5b13497ed2b2b66af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41890994eb7948c8a50eb7af60a70aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5357fa803e48d48c30f95dd694bcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dc7adc5188407a8a40b5aa41cdc90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Paths and parameters\n",
    "MODEL_NAME = \"facebook/opt-350m\"\n",
    "SAVE_DIR = \"./results\"\n",
    "TOP_LOW_SENSITIVITY_PERCENT = 30  # Focus on the least sensitive 30% layers\n",
    "NUM_CLUSTERS = 37  # Number of clusters for attention heads\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_SAMPLES = 100  # Limit for dataset size during evaluation\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = OPTForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def preprocess_data(dataset, max_samples=100, max_length=512):\n",
    "    \"\"\"\n",
    "    Preprocess dataset for tokenization.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        goal = example.get(\"goal\", \"\")\n",
    "        sol1 = example.get(\"sol1\", \"\")\n",
    "        sol2 = example.get(\"sol2\", \"\")\n",
    "        label = example.get(\"label\", None)  # Ground truth label\n",
    "        text = f\"{goal} {sol1} {sol2}\"\n",
    "        inputs.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "    )\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "\n",
    "def compute_sensitivity_scores(model, tokenized_inputs):\n",
    "    \"\"\"\n",
    "    Compute sensitivity scores for each attention head.\n",
    "    \"\"\"\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    sensitivity_scores = torch.zeros(num_layers, num_heads, device=DEVICE)\n",
    "\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        original_output = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    for layer in tqdm(range(num_layers), desc=\"Computing sensitivity scores\"):\n",
    "        for head in range(num_heads):\n",
    "            # Hook to zero out specific attention heads\n",
    "            def hook_fn(module, input, output):\n",
    "                # Debug: Print the output shape\n",
    "                print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "                # Handle different tensor shapes\n",
    "                if output.ndim == 4:\n",
    "                    # Standard attention output shape: (batch_size, num_heads, seq_length, head_dim)\n",
    "                    output[:, head, :, :] = 0  # Zero out the specific head\n",
    "                elif output.ndim == 3:\n",
    "                    # (batch_size, seq_length, embed_dim) - Heads are not explicitly separated\n",
    "                    # Compute the embedding size per head\n",
    "                    embed_dim = output.size(-1)\n",
    "                    head_dim = embed_dim // num_heads\n",
    "                    start_idx = head * head_dim\n",
    "                    end_idx = (head + 1) * head_dim\n",
    "                    output[:, :, start_idx:end_idx] = 0  # Zero out the specific head in embedding space\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected output shape: {output.shape}\")\n",
    "                return output\n",
    "            hook_handle = model.model.decoder.layers[layer].self_attn.out_proj.register_forward_hook(hook_fn)\n",
    "            # Forward pass with modified attention head\n",
    "            with torch.no_grad():\n",
    "                perturbed_output = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "            # Compute sensitivity as L2 norm of the difference\n",
    "            sensitivity_scores[layer, head] = torch.norm(original_output - perturbed_output, p=2)\n",
    "\n",
    "            hook_handle.remove()\n",
    "\n",
    "    return sensitivity_scores\n",
    "\n",
    "\n",
    "def identify_low_sensitivity_layers(sensitivity_scores, percent=TOP_LOW_SENSITIVITY_PERCENT):\n",
    "    \"\"\"\n",
    "    Identify layers with the lowest sensitivity scores.\n",
    "    \"\"\"\n",
    "    layer_scores = sensitivity_scores.mean(dim=1)  # Average sensitivity per layer\n",
    "    num_layers_to_keep = int(len(layer_scores) * percent / 100)\n",
    "    low_sensitivity_layers = torch.argsort(layer_scores)[:num_layers_to_keep]\n",
    "    return low_sensitivity_layers\n",
    "\n",
    "\n",
    "def extract_attention_keys(model, tokenized_inputs, low_sensitivity_layers):\n",
    "    \"\"\"\n",
    "    Extract attention keys for specified low-sensitivity layers.\n",
    "    \"\"\"\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"].to(DEVICE)\n",
    "    keys = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        keys.append(output.detach().cpu().numpy())  # Extract keys from self-attention\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for layer in low_sensitivity_layers:\n",
    "            hook_handle = model.model.decoder.layers[layer].self_attn.k_proj.register_forward_hook(hook_fn)\n",
    "            model(input_ids, attention_mask=attention_mask)\n",
    "            hook_handle.remove()\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "def cluster_attention_heads(keys, num_clusters=NUM_CLUSTERS):\n",
    "    \"\"\"\n",
    "    Cluster attention keys using KMeans.\n",
    "    \"\"\"\n",
    "    flattened_keys = [key.reshape(key.shape[0], -1) for key in keys]\n",
    "    all_keys = np.concatenate(flattened_keys, axis=0)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(all_keys)\n",
    "    return kmeans\n",
    "\n",
    "\n",
    "def apply_clustering_mask(model, low_sensitivity_layers, cluster_centers, cluster_threshold=5):\n",
    "    \"\"\"\n",
    "    Modify the model to zero out attention heads based on clustering results.\n",
    "    \"\"\"\n",
    "    def cluster_mask_fn(module, input, output):\n",
    "        # Zero out heads based on cluster proximity\n",
    "        cluster_mask = (cluster_centers.mean(axis=0) < cluster_threshold).astype(int)\n",
    "        output[:, cluster_mask == 0, :, :] = 0\n",
    "        return output\n",
    "\n",
    "    for layer in low_sensitivity_layers:\n",
    "        model.model.decoder.layers[layer].self_attn.out_proj.register_forward_hook(cluster_mask_fn)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenized_inputs, labels):\n",
    "    \"\"\"\n",
    "    Evaluate the model's accuracy and inference time.\n",
    "    \"\"\"\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    start_time = time.time()\n",
    "    correct = 0\n",
    "\n",
    "    for i in tqdm(range(len(labels)), desc=\"Evaluating model\"):\n",
    "        input_id = input_ids[i].unsqueeze(0)  # Add batch dimension\n",
    "        attention_mask_id = attention_mask[i].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_id, attention_mask=attention_mask_id).logits\n",
    "\n",
    "        # Extract logits for the final position\n",
    "        final_token_logits = logits[0, -1, :]\n",
    "        option1_score = final_token_logits[tokenizer.convert_tokens_to_ids(\"1\")].item()\n",
    "        option2_score = final_token_logits[tokenizer.convert_tokens_to_ids(\"2\")].item()\n",
    "\n",
    "        prediction = 0 if option1_score > option2_score else 1\n",
    "        if prediction == labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = correct / len(labels)\n",
    "    inference_time = end_time - start_time\n",
    "    return accuracy, inference_time\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"piqa\", split=\"validation\")\n",
    "    tokenized_inputs, labels = preprocess_data(dataset, max_samples=MAX_SAMPLES)\n",
    "\n",
    "    # Step 1: Compute sensitivity scores\n",
    "    print(\"Computing sensitivity scores...\")\n",
    "    sensitivity_scores = compute_sensitivity_scores(model, tokenized_inputs)\n",
    "\n",
    "    # Step 2: Identify low-sensitivity layers\n",
    "    print(\"Identifying low-sensitivity layers...\")\n",
    "    low_sensitivity_layers = identify_low_sensitivity_layers(sensitivity_scores)\n",
    "\n",
    "    # Step 3: Extract attention keys from low-sensitivity layers\n",
    "    print(\"Extracting attention keys...\")\n",
    "    attention_keys = extract_attention_keys(model, tokenized_inputs, low_sensitivity_layers)\n",
    "\n",
    "    # Step 4: Cluster attention heads\n",
    "    print(\"Clustering attention heads...\")\n",
    "    kmeans = cluster_attention_heads(attention_keys)\n",
    "\n",
    "    # Evaluate baseline model\n",
    "    print(\"Evaluating baseline model...\")\n",
    "    baseline_accuracy, baseline_time = evaluate_model(model, tokenized_inputs, labels)\n",
    "\n",
    "    # Apply clustering mask to the model\n",
    "    print(\"Evaluating clustered model...\")\n",
    "    clustered_model = apply_clustering_mask(model, low_sensitivity_layers, kmeans.cluster_centers_)\n",
    "    clustered_accuracy, clustered_time = evaluate_model(clustered_model, tokenized_inputs, labels)\n",
    "\n",
    "    # Compute speedup\n",
    "    speedup = baseline_time / clustered_time if clustered_time > 0 else 0\n",
    "\n",
    "    # Save and display results\n",
    "    print(\"Saving results...\")\n",
    "    results = {\n",
    "        \"Baseline Accuracy\": baseline_accuracy,\n",
    "        \"Baseline Time (s)\": baseline_time,\n",
    "        \"Clustered Accuracy\": clustered_accuracy,\n",
    "        \"Clustered Time (s)\": clustered_time,\n",
    "        \"Speedup\": speedup,\n",
    "    }\n",
    "    results_path = os.path.join(SAVE_DIR, \"evaluation_results.txt\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    print(\"Results saved:\", results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b8c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
