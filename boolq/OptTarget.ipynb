{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada7172b-cde3-4e2b-8836-6402abdbf309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d7517-c9e2-41a8-baea-870c9f543dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chai target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b64022-e160-4217-b97a-c4d20fd004df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV Cache Size: 14155776 elements\n",
      "Initial Accuracy: 0.6900\n",
      "Highly Sensitive Layers: [0, 1, 2, 3, 5]\n",
      "Epoch 1/3, Loss: 8.3624\n",
      "Epoch 2/3, Loss: 5.8478\n",
      "Epoch 3/3, Loss: 3.7370\n",
      "Final Accuracy After Fine-Tuning: 1.0000\n",
      "Accuracy Improvement: 0.3100\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import AutoTokenizer, OPTForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Paths and parameters\n",
    "MODEL_NAME = \"facebook/opt-350m\"  # Change to appropriate OPT model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLUSTERS = 2000  # Number of clusters for attention heads\n",
    "MAX_SAMPLES = 100  # Limit for dataset size during evaluation\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = OPTForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def calculate_kv_cache_size(model):\n",
    "    \"\"\"\n",
    "    Calculate the total size of K and V projection weights for the model.\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    for layer in range(model.config.num_hidden_layers):\n",
    "        attention_layer = model.model.decoder.layers[layer].self_attn\n",
    "        k_proj_size = attention_layer.k_proj.weight.numel()\n",
    "        v_proj_size = attention_layer.v_proj.weight.numel()\n",
    "        total_size += k_proj_size + v_proj_size\n",
    "    return total_size\n",
    "\n",
    "def evaluate_model(model, tokenized_inputs, labels):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataset and return accuracy.\n",
    "    \"\"\"\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    return accuracy_score(labels.cpu().numpy(), predictions)\n",
    "\n",
    "def preprocess_data(dataset, max_samples=100, max_length=512):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and return tokenized inputs and labels.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "\n",
    "        # Extract text input based on dataset structure\n",
    "        if \"question\" in example:  # For BoolQ-like datasets\n",
    "            text = f\"Question: {example['question']} Context: {example.get('context', example.get('passage', ''))}\"\n",
    "        elif \"ctx\" in example and \"endings\" in example:  # For HellaSwag\n",
    "            text = f\"Context: {example['ctx']} Ending: {example['endings'][0]}\"  # Using the first ending\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dataset format or missing keys.\")\n",
    "\n",
    "        # Extract the label dynamically\n",
    "        if \"answer\" in example:  # BoolQ-like datasets\n",
    "            label = int(example[\"answer\"])  # Convert boolean to integer (True=1, False=0)\n",
    "        elif \"label\" in example:\n",
    "            label = example[\"label\"]\n",
    "        elif \"gold_label\" in example:\n",
    "            label = example[\"gold_label\"]\n",
    "        else:\n",
    "            label = None  # Default if no valid label is found\n",
    "\n",
    "        if label is None:\n",
    "            print(f\"Skipping example due to missing label: {example}\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "        inputs.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "    )\n",
    "    return tokenized_inputs, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def cluster_attention_heads(model, num_clusters):\n",
    "    \"\"\"\n",
    "    Cluster attention heads based on synthetic or extracted key and value weights.\n",
    "    \"\"\"\n",
    "    cluster_centers_keys = []\n",
    "    cluster_centers_values = []\n",
    "\n",
    "    for layer in range(model.config.num_hidden_layers):\n",
    "        # Generate synthetic data for keys and values (placeholder for actual data)\n",
    "        k_data = np.random.rand(NUM_CLUSTERS, model.config.hidden_size)\n",
    "        v_data = np.random.rand(NUM_CLUSTERS, model.config.hidden_size)\n",
    "\n",
    "        # Perform clustering for keys\n",
    "        kmeans_keys = KMeans(n_clusters=num_clusters, random_state=42).fit(k_data)\n",
    "        cluster_centers_keys.append(kmeans_keys.cluster_centers_)\n",
    "\n",
    "        # Perform clustering for values\n",
    "        kmeans_values = KMeans(n_clusters=num_clusters, random_state=42).fit(v_data)\n",
    "        cluster_centers_values.append(kmeans_values.cluster_centers_)\n",
    "\n",
    "    return cluster_centers_keys, cluster_centers_values\n",
    "\n",
    "def identify_sensitive_layers(model):\n",
    "    \"\"\"\n",
    "    Identify highly sensitive layers based on their importance (e.g., attention scores).\n",
    "    \"\"\"\n",
    "    sensitivity_scores = []\n",
    "    for layer in range(model.config.num_hidden_layers):\n",
    "        attention_layer = model.model.decoder.layers[layer].self_attn\n",
    "        # Calculate sensitivity as the standard deviation of the projection weights\n",
    "        sensitivity_score = attention_layer.k_proj.weight.std().item()\n",
    "        sensitivity_scores.append(sensitivity_score)\n",
    "\n",
    "    # Determine a threshold for sensitivity\n",
    "    mean_sensitivity = np.mean(sensitivity_scores)\n",
    "    sensitive_layers = [i for i, score in enumerate(sensitivity_scores) if score > mean_sensitivity]\n",
    "    return sensitive_layers\n",
    "\n",
    "\n",
    "def fine_tune_model(model, dataloader, layers_to_finetune, num_epochs=3, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Fine-tune the model on specific layers after clustering.\n",
    "    \"\"\"\n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # If no sensitive layers are identified, default to fine-tuning the last layer\n",
    "    if not layers_to_finetune:\n",
    "        layers_to_finetune = [model.config.num_hidden_layers - 1]  # Fine-tune the last layer\n",
    "\n",
    "    # Unfreeze specific layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(f\"layers.{layer}.self_attn\" in name for layer in layers_to_finetune):\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0].to(DEVICE)\n",
    "            labels = batch[1].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).logits\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "def process_dataset_with_finetuning(dataset_name):\n",
    "    \"\"\"\n",
    "    Process the dataset, cluster layers, and fine-tune at targeted stages.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "    tokenized_inputs, labels = preprocess_data(dataset, max_samples=MAX_SAMPLES)\n",
    "\n",
    "    # DataLoader for fine-tuning\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        tokenized_inputs[\"input_ids\"], labels\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Initial evaluation\n",
    "    initial_kv_cache_size = calculate_kv_cache_size(model)\n",
    "    initial_accuracy = evaluate_model(model, tokenized_inputs, labels)\n",
    "    print(f\"Initial KV Cache Size: {initial_kv_cache_size} elements\")\n",
    "    print(f\"Initial Accuracy: {initial_accuracy:.4f}\")\n",
    "\n",
    "    # Perform clustering\n",
    "    cluster_centers_keys, cluster_centers_values = cluster_attention_heads(model, NUM_CLUSTERS)\n",
    "\n",
    "    # Identify and fine-tune highly sensitive layers\n",
    "    sensitive_layers = identify_sensitive_layers(model)\n",
    "    print(f\"Highly Sensitive Layers: {sensitive_layers}\")\n",
    "    fine_tune_model(model, train_loader, sensitive_layers, num_epochs=3)\n",
    "\n",
    "    # Reevaluate after fine-tuning\n",
    "    final_accuracy = evaluate_model(model, tokenized_inputs, labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy_improvement = final_accuracy - initial_accuracy\n",
    "    print(f\"Final Accuracy After Fine-Tuning: {final_accuracy:.4f}\")\n",
    "    print(f\"Accuracy Improvement: {accuracy_improvement:.4f}\")\n",
    "\n",
    "def main_with_finetuning():\n",
    "    for dataset_name in [\"boolq\"]:\n",
    "        process_dataset_with_finetuning(dataset_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_finetuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a86fa-ec4f-43b8-be02-e257ca2d2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chai benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdbcdc-d084-4163-ba41-decbaa6f9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import AutoTokenizer, OPTForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "\n",
    "# Paths and parameters\n",
    "MODEL_NAME = \"facebook/opt-350m\"  # Change to appropriate OPT model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLUSTERS = 2000  # Number of clusters for attention heads\n",
    "MAX_SAMPLES = 100  # Limit for dataset size during evaluation\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = OPTForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def calculate_layer_sensitivities(model, tokenized_inputs):\n",
    "    \"\"\"\n",
    "    Calculate sensitivities for each layer based on attention outputs.\n",
    "    Sensitivity is approximated by the variance of attention scores across heads.\n",
    "    \"\"\"\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"].to(DEVICE)\n",
    "    sensitivities = []\n",
    "\n",
    "    def attention_hook(module, input, output):\n",
    "        # Extract attention scores (usually the first element of the tuple)\n",
    "        if isinstance(output, tuple):\n",
    "            attention_scores = output[0]  # Assuming the first element is the attention scores\n",
    "        else:\n",
    "            attention_scores = output\n",
    "\n",
    "        # Convert to numpy\n",
    "        attention_scores = attention_scores.detach().cpu().numpy()\n",
    "        print(f\"Attention scores shape: {attention_scores.shape}\")  # Debug print\n",
    "\n",
    "        # Adjust variance calculation based on shape\n",
    "        if attention_scores.ndim == 4:  # Expected shape: [batch, heads, seq_len, seq_len]\n",
    "            variance = np.var(attention_scores, axis=(1, 2, 3))  # Across batch, heads, and seq\n",
    "        elif attention_scores.ndim == 3:  # Shape: [batch, seq_len, seq_len]\n",
    "            variance = np.var(attention_scores, axis=(1, 2))  # Across batch and seq\n",
    "        elif attention_scores.ndim == 2:  # Shape: [seq_len, seq_len]\n",
    "            variance = np.var(attention_scores, axis=(0, 1))  # Across seq\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected attention scores shape: {attention_scores.shape}\")\n",
    "\n",
    "        sensitivities.append(np.mean(variance))\n",
    "\n",
    "    hooks = []\n",
    "    for layer in range(len(model.model.decoder.layers)):\n",
    "        hook = model.model.decoder.layers[layer].self_attn.register_forward_hook(attention_hook)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def calculate_kv_cache_size_final(model, num_clusters):\n",
    "    \"\"\"\n",
    "    Calculate the total size of K and V projection weights for the clustered model.\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    head_dim = model.config.hidden_size // model.config.num_attention_heads  # Per-head dimension\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Each cluster contributes one set of weights for keys and values\n",
    "        k_proj_size = num_clusters * head_dim\n",
    "        v_proj_size = num_clusters * head_dim\n",
    "        total_size += k_proj_size + v_proj_size\n",
    "\n",
    "    return total_size\n",
    "def cluster_attention_heads(model, num_clusters):\n",
    "    \"\"\"\n",
    "    Cluster attention heads based on their sensitivities and outputs cluster centers.\n",
    "    \"\"\"\n",
    "    cluster_centers_keys = []\n",
    "    cluster_centers_values = []\n",
    "\n",
    "    for layer in range(model.config.num_hidden_layers):\n",
    "        # Generate synthetic data for keys and values (placeholder for actual data)\n",
    "        k_data = np.random.rand(NUM_CLUSTERS, model.config.hidden_size)\n",
    "        v_data = np.random.rand(NUM_CLUSTERS, model.config.hidden_size)\n",
    "\n",
    "        # Perform clustering for keys\n",
    "        kmeans_keys = KMeans(n_clusters=num_clusters, random_state=42).fit(k_data)\n",
    "        cluster_centers_keys.append(kmeans_keys.cluster_centers_)\n",
    "\n",
    "        # Perform clustering for values\n",
    "        kmeans_values = KMeans(n_clusters=num_clusters, random_state=42).fit(v_data)\n",
    "        cluster_centers_values.append(kmeans_values.cluster_centers_)\n",
    "\n",
    "    return cluster_centers_keys, cluster_centers_values\n",
    "\n",
    "\n",
    "def preprocess_data(dataset, max_samples=100, max_length=512):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "\n",
    "        # Extract text input based on dataset structure\n",
    "        if \"question\" in example:  # For BoolQ-like datasets\n",
    "            text = f\"Question: {example['question']} Context: {example.get('context', example.get('passage', ''))}\"\n",
    "        elif \"ctx\" in example and \"endings\" in example:  # For HellaSwag\n",
    "            text = f\"Context: {example['ctx']} Ending: {example['endings'][0]}\"  # Using the first ending\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dataset format or missing keys.\")\n",
    "\n",
    "        # Extract the label dynamically\n",
    "        if \"answer\" in example:  # BoolQ-like datasets\n",
    "            label = int(example[\"answer\"])  # Convert boolean to integer (True=1, False=0)\n",
    "        elif \"label\" in example:\n",
    "            label = example[\"label\"]\n",
    "        elif \"gold_label\" in example:\n",
    "            label = example[\"gold_label\"]\n",
    "        else:\n",
    "            label = None  # Default if no valid label is found\n",
    "\n",
    "        if label is None:\n",
    "            print(f\"Skipping example due to missing label: {example}\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "        inputs.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "    )\n",
    "    return tokenized_inputs, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenized_inputs, labels):\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    return accuracy_score(labels.cpu().numpy(), predictions)\n",
    "\n",
    "\n",
    "def process_dataset(dataset_name):\n",
    "    dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "    tokenized_inputs, labels = preprocess_data(dataset, max_samples=MAX_SAMPLES)\n",
    "\n",
    "    initial_accuracy = evaluate_model(model, tokenized_inputs, labels)\n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(f\"Initial Accuracy: {initial_accuracy:.4f}\")\n",
    "\n",
    "    sensitivities = calculate_layer_sensitivities(model, tokenized_inputs)\n",
    "    cluster_centers_keys, cluster_centers_values = cluster_attention_heads(model, NUM_CLUSTERS)\n",
    "\n",
    "    print(f\"Cluster centers (keys): {np.array(cluster_centers_keys).shape}\")\n",
    "    print(f\"Cluster centers (values): {np.array(cluster_centers_values).shape}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for dataset_name in [\"boolq\"]:\n",
    "        process_dataset(dataset_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
