{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375cc763-0f6f-4da3-9dcf-5f6eee6b5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 21:29:49.601639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-15 21:29:49.601683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-15 21:29:49.602877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 21:29:49.610390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_opt_classifier(model_name):\n",
    "    \"\"\"Load the specified OPT model and tokenizer.\"\"\"\n",
    "    model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_model_size(model, path=\"temp_model.pth\"):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    os.remove(path)\n",
    "    return size_mb\n",
    "def enforce_head_constraint(num_heads, embed_dim):\n",
    "    \"\"\" Adjusts number of heads to ensure divisibility with embedding dimension. \"\"\"\n",
    "    while embed_dim % num_heads != 0:\n",
    "        num_heads -= 1\n",
    "    return num_heads\n",
    "\n",
    "def load_opt_classifier():\n",
    "    model = OPTForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "    return model, tokenizer\n",
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\" Extracts attention scores from the model while ensuring correct dimensions. \"\"\"\n",
    "    attention_scores = {}\n",
    "    input_ids = input_ids.to(device)  #  Move input IDs to GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "\n",
    "        for layer_idx, attn in enumerate(outputs.attentions):\n",
    "            attn = attn.cpu().numpy()  #  Move to CPU\n",
    "            attn = np.mean(attn, axis=(0, 2, 3)) if attn.ndim == 4 else np.mean(attn, axis=0)\n",
    "            attention_scores[layer_idx] = attn  #  Store correctly processed attention scores\n",
    "\n",
    "    return attention_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cluster_heads(attention_scores, num_clusters):\n",
    "    \"\"\" Clusters attention heads while ensuring correct shape. \"\"\"\n",
    "    num_heads = len(attention_scores)\n",
    "\n",
    "    if num_heads <= 10:\n",
    "        return list(range(num_heads))\n",
    "\n",
    "    attention_scores = np.array(attention_scores).reshape(-1, 1)  #  Flatten for clustering\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=\"auto\")\n",
    "    kmeans.fit(attention_scores)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    cluster_representatives = []\n",
    "\n",
    "    for cluster_idx in range(num_clusters):\n",
    "        indices = np.where(labels == cluster_idx)[0]\n",
    "        if len(indices) > 0:\n",
    "            keep_count = max(1, len(indices) * 5 // 10)  # Pruning 50% of heads per cluster\n",
    "            cluster_representatives.extend(indices[:keep_count])\n",
    "\n",
    "    return sorted(cluster_representatives)\n",
    "\n",
    "def prune_attention_heads(model, clustered_heads):\n",
    "    \"\"\" Prunes attention heads while ensuring correct embedding dimensions. \"\"\"\n",
    "    for layer_idx, heads_to_keep in enumerate(clustered_heads):\n",
    "        attn_layer = model.model.decoder.layers[layer_idx].self_attn\n",
    "\n",
    "        #  Ensure valid number of heads per layer\n",
    "        original_num_heads = attn_layer.num_heads\n",
    "        new_num_heads = enforce_head_constraint(len(heads_to_keep), attn_layer.embed_dim)\n",
    "\n",
    "        #  Update number of heads\n",
    "        attn_layer.num_heads = new_num_heads\n",
    "\n",
    "        #  Ensure Q, K, V projections match new number of heads\n",
    "        head_dim = attn_layer.embed_dim // original_num_heads\n",
    "        new_embed_dim = new_num_heads * head_dim\n",
    "\n",
    "        attn_layer.q_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "        attn_layer.k_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "        attn_layer.v_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "\n",
    "        #  Ensure output projection layer matches new size\n",
    "        attn_layer.out_proj = nn.Linear(new_embed_dim, attn_layer.embed_dim, bias=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def divide_layers_by_sensitivity(sensitivities):\n",
    "    \"\"\" Splits layers into 3 groups (High, Medium, Low) based on sensitivity scores. \"\"\"\n",
    "    sorted_layers = sorted(sensitivities, key=sensitivities.get, reverse=True)\n",
    "    num_layers = len(sorted_layers)\n",
    "    high = sorted_layers[: int(num_layers * 0.2)]\n",
    "    medium = sorted_layers[int(num_layers * 0.2) : int(num_layers * 0.7)]\n",
    "    low = sorted_layers[int(num_layers * 0.7) :]\n",
    "    return high, medium, low\n",
    "\n",
    "def apply_mixed_precision(model, medium, low):\n",
    "    \"\"\" Applies mixed precision quantization to the model. \"\"\"\n",
    "    for layer_idx in medium + low:\n",
    "        for param in model.model.decoder.layers[layer_idx].parameters():\n",
    "            param.data = param.data.half()\n",
    "    model.half()\n",
    "    return model\n",
    "\n",
    "def compute_sensitivity(attention_scores):\n",
    "    # Debugging: Print attention scores\n",
    "    print(\"Raw Attention Scores:\", attention_scores)\n",
    "\n",
    "    # Compute absolute mean for each layer\n",
    "    cleaned_scores = {\n",
    "        layer_idx: np.mean(np.abs(np.array(scores, dtype=np.float32)))\n",
    "        for layer_idx, scores in attention_scores.items()\n",
    "        if isinstance(scores, (list, np.ndarray)) and len(scores) > 0  # Ensure non-empty values\n",
    "    }\n",
    "    \n",
    "    # Debugging: Print computed sensitivities\n",
    "    print(\"Computed Sensitivities:\", cleaned_scores)\n",
    "    \n",
    "    return cleaned_scores# -------------------- Evaluation -------------------- #\n",
    "# def evaluate_model(model, tokenizer):\n",
    "#     \"\"\" Evaluates model accuracy on PIQA dataset (multiple-choice task). \"\"\"\n",
    "#     dataset = load_dataset(\"piqa\", split=\"validation[:100]\")\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for example in dataset:\n",
    "#             prompt = example[\"goal\"]\n",
    "#             choices = [example[\"sol1\"], example[\"sol2\"]]\n",
    "#             inputs = tokenizer([prompt + \" \" + choice for choice in choices], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits.squeeze()\n",
    "#             predicted_choice = torch.argmax(logits).item()\n",
    "#             correct += (predicted_choice == example[\"label\"])\n",
    "#             total += 1\n",
    "\n",
    "#     return (correct / total) * 100\n",
    "\n",
    "# -------------------- Main Execution -------------------- #\n",
    "def get_optimal_clusters(attention_scores):\n",
    "    \"\"\" Determines optimal clusters for attention heads using the Elbow Method. \"\"\"\n",
    "    num_heads = len(attention_scores)\n",
    "    if num_heads <= 10:\n",
    "        return num_heads\n",
    "    max_clusters = max(num_heads - int(0.2 * num_heads), num_heads * 4 // 5)\n",
    "    errors = []\n",
    "    cluster_range = range(1, max_clusters + 1)\n",
    "    for num_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=\"auto\")\n",
    "        kmeans.fit(attention_scores.reshape(-1, 1))\n",
    "        errors.append(kmeans.inertia_)\n",
    "    elbow = KneeLocator(cluster_range, errors, curve=\"convex\", direction=\"decreasing\")\n",
    "    return max(num_heads - int(0.2 * num_heads), elbow.elbow if elbow.elbow else max_clusters)\n",
    "\n",
    "def get_model_size(model, path=\"temp_model.pth\"):\n",
    "    \"\"\" Saves model temporarily and checks disk size. \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)  # Convert bytes to MB\n",
    "    os.remove(path)  #  Clean up after measurement\n",
    "    return size_mb\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"sentence\"),\n",
    "        \"piqa\": (\"piqa\", \"train\", \"goal\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"sentence1\"),\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=\"train\", trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            inputs = tokenizer(example[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions.item() == example[\"label\"])\n",
    "            total += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "    return accuracy, latency\n",
    "\n",
    "def apply_pruning(model, tokenizer,dataset_name):\n",
    "    size_before = get_model_size(model)\n",
    "\n",
    "    print(\"\\n Evaluating Accuracy Before Any Modification...\")\n",
    "    # accuracy_before = evaluate_model(model, tokenizer,dataset_name)\n",
    "    # print(f\" Original Accuracy: {accuracy_before:.2f}%\\n\")\n",
    "\n",
    "    #  Measure Original Model Size\n",
    "    #  Compute Attention Scores\n",
    "    print(\"\\n Computing Attention Scores...\")\n",
    "    input_ids = torch.randint(0, 50256, (1, 32))  # Random input for attention extraction\n",
    "    attention_scores = get_attention_scores(model, input_ids)\n",
    "\n",
    "    #  Apply Clustering to All Layers\n",
    "    print(\"\\n Clustering Attention Heads...\")\n",
    "#  Ensure layers exist before accessing\n",
    "    if not attention_scores:\n",
    "        raise ValueError(\" No attention scores extracted! Check if model supports output_attentions.\")\n",
    "\n",
    "    available_layers = list(attention_scores.keys())\n",
    "    print(f\" Available Layers for Clustering: {available_layers}\")\n",
    "\n",
    "    clustered_heads = [\n",
    "        cluster_heads(attention_scores[layer], get_optimal_clusters(attention_scores[layer]))\n",
    "        for layer in available_layers\n",
    "    ]\n",
    "\n",
    "\n",
    "    #  Apply Clustering & Pruning (CHAI-Base)\n",
    "    print(\"\\n Applying Clustering and Pruning (CHAI-Base)...\")\n",
    "    chai_base_model = prune_attention_heads(model, clustered_heads)\n",
    "    print(\"got heads\")\n",
    "    return chai_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f4d095-9bf0-43fa-9d12-f9da12b2b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\"Extracts attention scores from the model while ensuring correct dimensions.\"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = input_ids.to(device)  #  Move input IDs to GPU\n",
    "    model.to(device)\n",
    "    \n",
    "    attention_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        if hasattr(outputs, \"attentions\") and outputs.attentions is not None:\n",
    "            for layer_idx, attn in enumerate(outputs.attentions):\n",
    "                attn = attn.cpu().numpy()  #  Move to CPU\n",
    "                if attn.ndim == 4:  # Expected shape: (batch_size, num_heads, seq_length, seq_length)\n",
    "                    attn = np.mean(attn, axis=(0, 2, 3))  #  Average across heads and sequences\n",
    "                elif attn.ndim == 3:  # Unexpected case, still averaging\n",
    "                    attn = np.mean(attn, axis=(0, 2))\n",
    "                elif attn.ndim == 2:  # More unexpected cases\n",
    "                    attn = np.mean(attn, axis=0)\n",
    "                \n",
    "                attention_scores[layer_idx] = attn  # Store correctly processed attention scores\n",
    "\n",
    "        else:\n",
    "            return {\"error\": \"Model does not return attention scores. Check model architecture.\"}\n",
    "\n",
    "    return attention_scores\n",
    "\n",
    "def divide_layers_by_sensitivity(sensitivities):\n",
    "    \"\"\" Splits layers into 3 groups (High, Medium, Low) based on sensitivity scores. \"\"\"\n",
    "    sorted_layers = sorted(sensitivities, key=sensitivities.get, reverse=True)\n",
    "    num_layers = len(sorted_layers)\n",
    "    high = sorted_layers[: int(num_layers * 0.2)]\n",
    "    medium = sorted_layers[int(num_layers * 0.2) : int(num_layers * 0.7)]\n",
    "    low = sorted_layers[int(num_layers * 0.7) :]\n",
    "    return high, medium, low\n",
    "\n",
    "def apply_mixed_precision(model, medium, low):\n",
    "    \"\"\" Applies mixed precision quantization to the model. \"\"\"\n",
    "    for layer_idx in medium + low:\n",
    "        for param in model.model.decoder.layers[layer_idx].parameters():\n",
    "            param.data = param.data.half()\n",
    "    model.half()\n",
    "    return model\n",
    "\n",
    "def compute_sensitivity(attention_scores):\n",
    "    cleaned_scores = {\n",
    "        layer_idx: np.mean(np.abs(np.array(scores, dtype=np.float32)))\n",
    "        for layer_idx, scores in attention_scores.items()\n",
    "        if isinstance(scores, (list, np.ndarray))\n",
    "    }\n",
    "    return cleaned_scores\n",
    "\n",
    "# -------------------- Evaluation -------------------- #\n",
    "\n",
    "def chai_quant_enhancement(chai_base_model,tokenizer,dataset_name):\n",
    "    #  Save & Reload to Apply Pruning\n",
    "    #  Measure Model Size After Pruning\n",
    "    input_ids = torch.randint(0, 50256, (1, 32))\n",
    "    attention_scores = get_attention_scores(chai_base_model, input_ids)\n",
    "    sensitivities = compute_sensitivity(attention_scores)\n",
    "    high, medium, low = divide_layers_by_sensitivity(sensitivities)\n",
    "\n",
    "    #  Apply Mixed Precision Quantization (CHAI-Quant)\n",
    "    print(\"\\n🚀 Applying Mixed Precision Quantization (CHAI-Quant)...\")\n",
    "    chai_quant_model = apply_mixed_precision(chai_base_model, medium, low)\n",
    "    print(\" [chai-quant] After modification: model parameters\")\n",
    "    for name, param in chai_quant_model.named_parameters():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "    return chai_quant_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f2e7ed-e530-4442-b0aa-9f58cf9ab883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d86d7df26244bc88c723e6f9fcdd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4258231fd8534dbb93e4f95f29f2d2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311ca8c310b340b2b886887a116f9773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19077fa2149d4c848136fd3ab67fd9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075479187efb4182ae3a7f88af7311c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd7e0842a7944578b421c3337abf8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Applying Mixed Precision Quantization (CHAI-Quant)...\n",
      "✅ [chai-quant] After modification: model parameters\n",
      "  model.decoder.embed_tokens.weight: torch.Size([50272, 512])\n",
      "  model.decoder.embed_positions.weight: torch.Size([2050, 1024])\n",
      "  model.decoder.project_out.weight: torch.Size([512, 1024])\n",
      "  model.decoder.project_in.weight: torch.Size([1024, 512])\n",
      "  model.decoder.layers.0.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.0.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.0.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.0.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.0.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.0.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.0.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.0.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.1.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.1.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.1.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.1.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.1.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.1.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.1.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.2.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.2.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.2.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.2.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.2.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.2.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.2.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.3.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.3.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.3.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.3.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.3.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.3.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.3.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.4.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.4.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.4.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.4.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.4.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.4.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.4.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.5.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.5.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.5.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.5.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.5.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.5.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.5.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.6.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.6.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.6.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.6.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.6.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.6.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.6.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.7.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.7.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.7.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.7.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.7.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.7.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.7.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.8.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.8.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.8.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.8.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.8.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.8.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.8.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.9.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.9.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.9.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.9.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.9.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.9.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.9.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.10.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.10.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.10.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.10.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.10.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.10.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.10.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.11.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.11.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.11.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.11.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.11.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.11.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.11.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.12.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.12.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.12.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.12.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.12.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.12.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.12.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.13.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.13.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.13.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.13.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.13.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.13.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.13.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.14.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.14.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.14.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.14.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.14.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.14.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.14.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.15.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.15.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.15.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.15.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.15.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.15.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.15.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.16.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.16.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.16.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.16.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.16.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.16.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.16.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.17.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.17.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.17.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.17.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.17.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.17.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.17.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.18.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.18.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.18.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.18.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.18.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.18.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.18.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.19.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.19.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.19.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.19.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.19.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.19.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.19.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.20.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.20.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.20.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.20.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.20.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.20.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.20.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.21.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.21.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.21.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.21.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.21.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.21.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.21.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.22.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.22.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.22.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.22.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.22.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.22.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.22.final_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.k_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.k_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.v_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.v_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.q_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.q_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
      "  model.decoder.layers.23.self_attn.out_proj.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.23.self_attn_layer_norm.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.fc1.weight: torch.Size([4096, 1024])\n",
      "  model.decoder.layers.23.fc1.bias: torch.Size([4096])\n",
      "  model.decoder.layers.23.fc2.weight: torch.Size([1024, 4096])\n",
      "  model.decoder.layers.23.fc2.bias: torch.Size([1024])\n",
      "  model.decoder.layers.23.final_layer_norm.weight: torch.Size([1024])\n",
      "  model.decoder.layers.23.final_layer_norm.bias: torch.Size([1024])\n",
      "  score.weight: torch.Size([2, 512])\n",
      "applied_methods.append(Quantization)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfbd75f78ec482aaf066203b6082668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d261af97ab942f6bdc00c96238345f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0d40eafa76427fabe430efbb3101ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d085f9644de343878c0ac22b8495bd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43b91b837a47be8aa79be45923e930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dca171c1d74a969e3574dd65b2d233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207acc6e16ec4c34bd7ce122853cb452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.089132726543816\n",
      "879.7333812713623\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"piqa\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "[a1, b1] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "\n",
    "\n",
    "#  Apply Pruning\n",
    "model = apply_pruning(model, tokenizer, dataset_name)\n",
    "\n",
    "print(\"\\n Initial Model Evaluation:\")\n",
    "print(f\"Accuracy: {a1:.2f}%\")\n",
    "print(f\"Latency: {b1:.4f} sec\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#  Measure Size After Pruning\n",
    "print(\"\\n Chai Base Model Evaluation (After Pruning)\")\n",
    "size2 = get_model_size(model)\n",
    "print(f\"Reduction Percentage: {(size1 - size2) * 100 / size1:.2f}%\")\n",
    "\n",
    "[a, b] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "print(f\"Accuracy: {a:.2f}%\")\n",
    "print(f\"Latency: {b:.4f} sec\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#  Apply CHAI-Quant Enhancement\n",
    "model = chai_quant_enhancement(model, tokenizer)\n",
    "size3 = get_model_size(model)\n",
    "print(\"\\n Applied Methods: Quantization\")\n",
    "[a, b] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "\n",
    "print(\"\\n Final Model Evaluation (After CHAI-Quant)\")\n",
    "print(f\"Accuracy after applying methods: {a:.2f}%\")\n",
    "print(f\"Latency: {b:.4f} sec\")\n",
    "print(f\"Reduction Percentage: {(size2 - size3) * 100 / size2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941836d-4b17-4fa4-9a55-ea012e9ce64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
