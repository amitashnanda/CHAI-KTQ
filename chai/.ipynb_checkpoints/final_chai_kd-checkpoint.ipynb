{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375cc763-0f6f-4da3-9dcf-5f6eee6b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_opt_classifier(model_name):\n",
    "    \"\"\"Load the specified OPT model and tokenizer.\"\"\"\n",
    "    model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_model_size(model, path=\"temp_model.pth\"):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    os.remove(path)\n",
    "    return size_mb\n",
    "def enforce_head_constraint(num_heads, embed_dim):\n",
    "    \"\"\" Adjusts number of heads to ensure divisibility with embedding dimension. \"\"\"\n",
    "    while embed_dim % num_heads != 0:\n",
    "        num_heads -= 1\n",
    "    return num_heads\n",
    "\n",
    "def load_opt_classifier():\n",
    "    model = OPTForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "    return model, tokenizer\n",
    "def get_attention_scores(model, input_ids):\n",
    "    \"\"\" Extracts attention scores from the model while ensuring correct dimensions. \"\"\"\n",
    "    attention_scores = {}\n",
    "    input_ids = input_ids.to(device)  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "\n",
    "        for layer_idx, attn in enumerate(outputs.attentions):\n",
    "            attn = attn.cpu().numpy()  \n",
    "            attn = np.mean(attn, axis=(0, 2, 3)) if attn.ndim == 4 else np.mean(attn, axis=0)\n",
    "            attention_scores[layer_idx] = attn  \n",
    "    return attention_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cluster_heads(attention_scores, num_clusters):\n",
    "    \"\"\" Clusters attention heads while ensuring correct shape. \"\"\"\n",
    "    num_heads = len(attention_scores)\n",
    "\n",
    "    if num_heads <= 10:\n",
    "        return list(range(num_heads))\n",
    "\n",
    "    attention_scores = np.array(attention_scores).reshape(-1, 1)  \n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=\"auto\")\n",
    "    kmeans.fit(attention_scores)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    cluster_representatives = []\n",
    "\n",
    "    for cluster_idx in range(num_clusters):\n",
    "        indices = np.where(labels == cluster_idx)[0]\n",
    "        if len(indices) > 0:\n",
    "            keep_count = max(1, len(indices) * 5 // 10)  \n",
    "            cluster_representatives.extend(indices[:keep_count])\n",
    "\n",
    "    return sorted(cluster_representatives)\n",
    "\n",
    "def prune_attention_heads(model, clustered_heads):\n",
    "    \"\"\" Prunes attention heads while ensuring correct embedding dimensions. \"\"\"\n",
    "    for layer_idx, heads_to_keep in enumerate(clustered_heads):\n",
    "        attn_layer = model.model.decoder.layers[layer_idx].self_attn\n",
    "\n",
    "        # Ensure valid number of heads per layer\n",
    "        original_num_heads = attn_layer.num_heads\n",
    "        new_num_heads = enforce_head_constraint(len(heads_to_keep), attn_layer.embed_dim)\n",
    "\n",
    "        #  Update number of heads\n",
    "        attn_layer.num_heads = new_num_heads\n",
    "\n",
    "        #  Ensure Q, K, V projections match new number of heads\n",
    "        head_dim = attn_layer.embed_dim // original_num_heads\n",
    "        new_embed_dim = new_num_heads * head_dim\n",
    "\n",
    "        attn_layer.q_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "        attn_layer.k_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "        attn_layer.v_proj = nn.Linear(attn_layer.embed_dim, new_embed_dim, bias=False)\n",
    "\n",
    "        #  Ensure output projection layer matches new size\n",
    "        attn_layer.out_proj = nn.Linear(new_embed_dim, attn_layer.embed_dim, bias=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def divide_layers_by_sensitivity(sensitivities):\n",
    "    \"\"\" Splits layers into 3 groups (High, Medium, Low) based on sensitivity scores. \"\"\"\n",
    "    sorted_layers = sorted(sensitivities, key=sensitivities.get, reverse=True)\n",
    "    num_layers = len(sorted_layers)\n",
    "    high = sorted_layers[: int(num_layers * 0.2)]\n",
    "    medium = sorted_layers[int(num_layers * 0.2) : int(num_layers * 0.7)]\n",
    "    low = sorted_layers[int(num_layers * 0.7) :]\n",
    "    return high, medium, low\n",
    "\n",
    "def apply_mixed_precision(model, medium, low):\n",
    "    \"\"\" Applies mixed precision quantization to the model. \"\"\"\n",
    "    for layer_idx in medium + low:\n",
    "        for param in model.model.decoder.layers[layer_idx].parameters():\n",
    "            param.data = param.data.half()\n",
    "    model.half()\n",
    "    return model\n",
    "\n",
    "def compute_sensitivity(attention_scores):\n",
    "    # Debugging: Print attention scores\n",
    "    print(\"Raw Attention Scores:\", attention_scores)\n",
    "\n",
    "    # Compute absolute mean for each layer\n",
    "    cleaned_scores = {\n",
    "        layer_idx: np.mean(np.abs(np.array(scores, dtype=np.float32)))\n",
    "        for layer_idx, scores in attention_scores.items()\n",
    "        if isinstance(scores, (list, np.ndarray)) and len(scores) > 0  # Ensure non-empty values\n",
    "    }\n",
    "    \n",
    "    # Debugging: Print computed sensitivities\n",
    "    print(\"Computed Sensitivities:\", cleaned_scores)\n",
    "    \n",
    "    return cleaned_scores# -------------------- Evaluation -------------------- #\n",
    "# def evaluate_model(model, tokenizer):\n",
    "#     \"\"\" Evaluates model accuracy on PIQA dataset (multiple-choice task). \"\"\"\n",
    "#     dataset = load_dataset(\"piqa\", split=\"validation[:100]\")\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for example in dataset:\n",
    "#             prompt = example[\"goal\"]\n",
    "#             choices = [example[\"sol1\"], example[\"sol2\"]]\n",
    "#             inputs = tokenizer([prompt + \" \" + choice for choice in choices], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits.squeeze()\n",
    "#             predicted_choice = torch.argmax(logits).item()\n",
    "#             correct += (predicted_choice == example[\"label\"])\n",
    "#             total += 1\n",
    "\n",
    "#     return (correct / total) * 100\n",
    "\n",
    "# -------------------- Main Execution -------------------- #\n",
    "def get_optimal_clusters(attention_scores):\n",
    "    \"\"\" Determines optimal clusters for attention heads using the Elbow Method. \"\"\"\n",
    "    num_heads = len(attention_scores)\n",
    "    if num_heads <= 10:\n",
    "        return num_heads\n",
    "    max_clusters = max(num_heads - int(0.2 * num_heads), num_heads * 4 // 5)\n",
    "    errors = []\n",
    "    cluster_range = range(1, max_clusters + 1)\n",
    "    for num_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=\"auto\")\n",
    "        kmeans.fit(attention_scores.reshape(-1, 1))\n",
    "        errors.append(kmeans.inertia_)\n",
    "    elbow = KneeLocator(cluster_range, errors, curve=\"convex\", direction=\"decreasing\")\n",
    "    return max(num_heads - int(0.2 * num_heads), elbow.elbow if elbow.elbow else max_clusters)\n",
    "\n",
    "def get_model_size(model, path=\"temp_model.pth\"):\n",
    "    \"\"\" Saves model temporarily and checks disk size. \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)  # Convert bytes to MB\n",
    "    os.remove(path)  #  Clean up after measurement\n",
    "    return size_mb\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"sentence\"),\n",
    "        \"piqa\": (\"piqa\", \"train\", \"goal\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"sentence1\"),\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=\"train\", trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            inputs = tokenizer(example[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions.item() == example[\"label\"])\n",
    "            total += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "    return accuracy, latency\n",
    "\n",
    "def apply_pruning(model, tokenizer,dataset_name):\n",
    "    size_before = get_model_size(model)\n",
    "\n",
    "    print(\"\\n Evaluating Accuracy Before Any Modification...\")\n",
    "    # accuracy_before = evaluate_model(model, tokenizer,dataset_name)\n",
    "    # print(f\" Original Accuracy: {accuracy_before:.2f}%\\n\")\n",
    "\n",
    "    #  Measure Original Model Size\n",
    "    #  Compute Attention Scores\n",
    "    print(\"\\n Computing Attention Scores...\")\n",
    "    input_ids = torch.randint(0, 50256, (1, 32))  # Random input for attention extraction\n",
    "    attention_scores = get_attention_scores(model, input_ids)\n",
    "\n",
    "    #  Apply Clustering to All Layers\n",
    "    print(\"\\n Clustering Attention Heads...\")\n",
    "#  Ensure layers exist before accessing\n",
    "    if not attention_scores:\n",
    "        raise ValueError(\" No attention scores extracted! Check if model supports output_attentions.\")\n",
    "\n",
    "    available_layers = list(attention_scores.keys())\n",
    "    print(f\" Available Layers for Clustering: {available_layers}\")\n",
    "\n",
    "    clustered_heads = [\n",
    "        cluster_heads(attention_scores[layer], get_optimal_clusters(attention_scores[layer]))\n",
    "        for layer in available_layers\n",
    "    ]\n",
    "\n",
    "\n",
    "    #  Apply Clustering & Pruning (CHAI-Base)\n",
    "    print(\"\\n Applying Clustering and Pruning (CHAI-Base)...\")\n",
    "    chai_base_model = prune_attention_heads(model, clustered_heads)\n",
    "    print(\"got heads\")\n",
    "    return chai_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc73912-6be1-499c-a5bb-520707ad0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import OPTForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "\n",
    "#  Detect and use the available device (MPS for Apple GPUs, CUDA for NVIDIA, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "def load_teacher_model(model_name, model_path):\n",
    "    \"\"\" Load the teacher model and move it to the correct device \"\"\"\n",
    "    print(f\" Loading teacher model {model_name} from {model_path}\")\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\" Model file not found at {model_path}\")\n",
    "\n",
    "    teacher_model = OPTForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    try:\n",
    "        teacher_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(\" Successfully loaded teacher model weights.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading teacher model weights: {e}\")\n",
    "\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    return teacher_model\n",
    "\n",
    "def chai_knowledge_distillation_enhancement(student_model, teacher_model, tokenizer, dataset_name):\n",
    "    \"\"\" Apply Knowledge Distillation (CHAI-KD) Enhancement \"\"\"\n",
    "    \n",
    "    print(\"\\n Applying Knowledge Distillation (CHAI-KD)...\")\n",
    "    print(\" [chai-kd] Before modification: model parameters\")\n",
    "    for name, param in student_model.named_parameters():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "    #  Move student model to the correct device\n",
    "    student_model.to(device)\n",
    "\n",
    "    #  Training hyperparameters\n",
    "    epochs = 1\n",
    "    batch_size = 16\n",
    "    temperature = 2.0\n",
    "    alpha = 0.5\n",
    "\n",
    "    #  Load dataset and tokenize it\n",
    "    dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:5000]\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./chai_kd_model\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    class KDTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            \"\"\" Compute Knowledge Distillation Loss \"\"\"\n",
    "\n",
    "            labels = inputs.pop(\"labels\")  # Extract labels\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}  #  Move inputs to the correct device\n",
    "            model.to(device)\n",
    "\n",
    "            student_outputs = model(**inputs)\n",
    "            student_logits = student_outputs.logits\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(**inputs)  # Ensure teacher model is on the correct device\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "\n",
    "            kd_loss = F.kl_div(\n",
    "                F.log_softmax(student_logits / temperature, dim=-1),\n",
    "                F.softmax(teacher_logits / temperature, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (temperature ** 2)\n",
    "\n",
    "            ce_loss = F.cross_entropy(student_logits, labels)\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss  # Combined loss\n",
    "\n",
    "            return (loss, student_outputs) if return_outputs else loss\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "\n",
    "    trainer = KDTrainer(\n",
    "        model=student_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator  #  Ensure proper padding\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\" [chai-kd] After modification: model parameters\")\n",
    "    for name, param in student_model.named_parameters():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "    return student_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb6938-5867-464b-8dd6-8670bfcc771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "\n",
    "    #  Dataset mappings for correct loading\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"validation\", \"sentence\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"validation\", (\"sentence1\", \"sentence2\")),  # RTE has two sentences\n",
    "        \"piqa\": (\"piqa\", None, \"validation\", (\"goal\", \"sol1\", \"sol2\")),  # PIQA has different format\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, split_name, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    #  Load dataset with cache handling\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=split_name, cache_dir=\"./cache\") if dataset_subset \\\n",
    "            else load_dataset(dataset_source, split=split_name, cache_dir=\"./cache\")\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    #  Enable GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure correct precision (avoid float16 on CPU)\n",
    "    if device.type == \"cpu\":\n",
    "        model.to(torch.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #  Handle multiple input fields correctly\n",
    "            if isinstance(text_key, tuple):  # RTE & PIQA\n",
    "                inputs = tokenizer(*[batch[key] for key in text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            else:\n",
    "                inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            # ✅ FIX: Correct tensor creation and movement\n",
    "            labels = torch.tensor(batch[\"label\"], dtype=torch.long).to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(F.softmax(outputs.logits, dim=-1), dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    return [accuracy, latency]\n",
    "\n",
    "dataset_name = \"sst2\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "# model = OPTForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model, tokenizer = load_opt_classifier(model_name)\n",
    "model = apply_pruning(model,tokenizer,dataset_name)\n",
    "classifier_model = kd_load_model_for_kd(model_name)\n",
    "\n",
    "\n",
    "#  Track applied methods\n",
    "def evaluate_model(model, tokenizer, dataset_name):\n",
    "    \"\"\"Evaluates model accuracy on the given dataset.\"\"\"\n",
    "\n",
    "    #  Dataset mappings for correct loading\n",
    "    dataset_mapping = {\n",
    "        \"sst2\": (\"glue\", \"sst2\", \"validation\", \"sentence\"),\n",
    "        \"rte\": (\"glue\", \"rte\", \"validation\", (\"sentence1\", \"sentence2\")),  # RTE has two sentences\n",
    "        \"piqa\": (\"piqa\", None, \"validation\", (\"goal\", \"sol1\", \"sol2\")),  # PIQA has different format\n",
    "    }\n",
    "\n",
    "    if dataset_name not in dataset_mapping:\n",
    "        return {\"error\": f\"Unsupported dataset: {dataset_name}\"}\n",
    "\n",
    "    dataset_source, dataset_subset, split_name, text_key = dataset_mapping[dataset_name]\n",
    "\n",
    "    #  Load dataset with cache handling\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_source, dataset_subset, split=split_name, cache_dir=\"./cache\") if dataset_subset \\\n",
    "            else load_dataset(dataset_source, split=split_name, cache_dir=\"./cache\")\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading dataset: {str(e)}\"}\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    #  Enable GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #  Ensure correct precision (avoid float16 on CPU)\n",
    "    if device.type == \"cpu\":\n",
    "        model.to(torch.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #  Handle multiple input fields correctly\n",
    "            if isinstance(text_key, tuple):  # RTE & PIQA\n",
    "                inputs = tokenizer(*[batch[key] for key in text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            else:\n",
    "                inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            #  FIX: Correct tensor creation and movement\n",
    "            labels = torch.tensor(batch[\"label\"], dtype=torch.long).to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(F.softmax(outputs.logits, dim=-1), dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    return [accuracy, latency]\n",
    "\n",
    "\n",
    "[a1, b1] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "\n",
    "# Apply Pruning\n",
    "model = apply_pruning(model, tokenizer, dataset_name)\n",
    "\n",
    "print(\"\\n Initial Model Evaluation:\")\n",
    "print(f\"Accuracy: {a1:.2f}%\")\n",
    "print(f\"Latency: {b1:.4f} sec\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#  Measure Size After Pruning\n",
    "print(\"\\n Chai Base Model Evaluation (After Pruning)\")\n",
    "size2 = get_model_size(model)\n",
    "print(f\"Reduction Percentage: {(size1 - size2) * 100 / size1:.2f}%\")\n",
    "\n",
    "[a, b] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "print(f\"Accuracy: {a:.2f}%\")\n",
    "print(f\"Latency: {b:.4f} sec\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "#  Apply CHAI-Quant Enhancement\n",
    "model = chai_knowledge_distillation_enhancement(model, tokenizer)\n",
    "size3 = get_model_size(model)\n",
    "print(\"\\n Applied Methods: KD\")\n",
    "[a, b] = evaluate_model_piqa(model, tokenizer,dataset, num_classes=2)\n",
    "\n",
    "print(\"\\n Final Model Evaluation (After CHAI-KD)\")\n",
    "print(f\"Accuracy after applying methods: {a:.2f}%\")\n",
    "print(f\"Latency: {b:.4f} sec\")\n",
    "print(f\"Reduction Percentage: {(size2 - size3) * 100 / size2:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
